{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "data_train = json.load(open('annotations/captions_train2014.json','r'))\n",
    "data_val = json.load(open('annotations/captions_val2014.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57870"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['images'][0]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "captions_train = json.load(open('../../vqa1/openEnded_mscoco_train2014_questions.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "captions_val = json.load(open('../../vqa1/openEnded_mscoco_val2014_questions.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'image_id': 487025,\n",
       " u'question': u'What shape is the bench seat?',\n",
       " u'question_id': 4870250}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_train['questions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = data_train['images']\n",
    "captions = []\n",
    "for cap in captions_train['questions']:\n",
    "    captions.append({'image_id':cap['image_id'],\n",
    "                     'id':cap['image_id'],\n",
    "                    'caption': cap['question']\n",
    "                    })\n",
    "\n",
    "with open(\"train_vqa.json\", \"w\") as outfile:\n",
    "    json.dump({'images':images, 'annotations':captions}, outfile, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = data_val['images']\n",
    "captions = []\n",
    "for cap in captions_val['questions']:\n",
    "    captions.append({'image_id':cap['image_id'],\n",
    "                     'id':cap['image_id'],\n",
    "                    'caption': cap['question']\n",
    "                    })\n",
    "\n",
    "with open(\"val_vqa.json\", \"w\") as outfile:\n",
    "    json.dump({'images':images, 'annotations':captions}, outfile, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = json.load(open('coco_raw_vqa.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'captions': u'What color is the wall?',\n",
       " u'file_path': u'val2014/COCO_val2014_000000118073.jpg',\n",
       " u'id': 118073}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[10002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "captions = []\n",
    "for i,caption in enumerate(data_train):\n",
    "    caption = caption.replace(' .','.')\n",
    "    images.append({'license':5,'file_name':\"train_\"+str(i),\n",
    "                   'coco_url':\"http://temp.org/images/\",\n",
    "                   'height':480,\n",
    "                   'width':640,\n",
    "                   'date_captured':\"2013-11-14 16:28:13\",\n",
    "                   'flickr_url':\"http://temp.org/images/\"+str(i),\n",
    "                   'id':i+1})\n",
    "    captions.append({'image_id':i+1,\n",
    "                     'id':i+1,\n",
    "                     'caption':caption})\n",
    "\n",
    "with open(\"train_vqa.json\", \"w\") as outfile:\n",
    "    json.dump({'images':images, 'annotations':captions}, outfile, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "captions = []\n",
    "for i,caption in enumerate(data_val):\n",
    "    caption = caption.replace(' .','.')\n",
    "    images.append({'license':5,'file_name':\"val_\"+str(i),\n",
    "                   'coco_url':\"http://temp.org/images/\",\n",
    "                   'height':480,\n",
    "                   'width':640,\n",
    "                   'date_captured':\"2013-11-14 16:28:13\",\n",
    "                   'flickr_url':\"http://temp.org/images/\"+str(i),\n",
    "                   'id':i+1})\n",
    "    captions.append({'image_id':i+1,\n",
    "                     'id':i+1,\n",
    "                     'caption':caption})\n",
    "\n",
    "\n",
    "with open(\"train_vqa.json\", \"w\") as outfile:\n",
    "    json.dump({'images':images, 'annotations':captions}, outfile, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coco_preprocess_newSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'images', u'annotations']\n",
      "40504\n",
      "121512\n",
      "{u'license': 3, u'file_name': u'COCO_val2014_000000391895.jpg', u'coco_url': u'http://mscoco.org/images/391895', u'height': 360, u'width': 640, u'date_captured': u'2013-11-14 11:18:45', u'flickr_url': u'http://farm9.staticflickr.com/8186/8119368305_4e622c8349_z.jpg', u'id': 391895}\n",
      "{u'image_id': 350623, u'id': 350623, u'caption': u'What is the table made of?'}\n",
      "[{'captions': u'Is the road paved?', 'file_path': u'val2014/COCO_val2014_000000391895.jpg', 'id': 391895}, {'captions': u'Is the lady holding the knife right?', 'file_path': u'val2014/COCO_val2014_000000522418.jpg', 'id': 522418}, {'captions': u'What animal is this?', 'file_path': u'val2014/COCO_val2014_000000184613.jpg', 'id': 184613}, {'captions': u'What brand is the keyboard?', 'file_path': u'val2014/COCO_val2014_000000318219.jpg', 'id': 318219}, {'captions': u'Does every person in the photo appear to be wearing headphones?', 'file_path': u'val2014/COCO_val2014_000000554625.jpg', 'id': 554625}, {'captions': u'Is the rolling pin being used?', 'file_path': u'val2014/COCO_val2014_000000397133.jpg', 'id': 397133}, {'captions': u'What color is the ladys dress?', 'file_path': u'val2014/COCO_val2014_000000574769.jpg', 'id': 574769}, {'captions': u\"Is that a watch on someone's hand?\", 'file_path': u'val2014/COCO_val2014_000000060623.jpg', 'id': 60623}, {'captions': u'Is the pot on the burner almost empty?', 'file_path': u'val2014/COCO_val2014_000000309022.jpg', 'id': 309022}, {'captions': u'Are the men about the same age?', 'file_path': u'val2014/COCO_val2014_000000005802.jpg', 'id': 5802}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "val = json.load(open('val_vqa.json', 'r'))\n",
    "train = json.load(open('train_vqa.json', 'r'))\n",
    "\n",
    "print val.keys()\n",
    "#print val['info']\n",
    "print len(val['images'])\n",
    "print len(val['annotations'])\n",
    "print val['images'][0]\n",
    "print val['annotations'][0]\n",
    "\n",
    "# combine all images and annotations together\n",
    "imgs = val['images'] + train['images']\n",
    "annots = val['annotations'] + train['annotations']\n",
    "\n",
    "# for efficiency lets group annotations by image\n",
    "itoa = {}\n",
    "for a in annots:\n",
    "    imgid = a['image_id']\n",
    "    if not imgid in itoa: itoa[imgid] = []\n",
    "    itoa[imgid].append(a)\n",
    "\n",
    "# create the json blob\n",
    "out = []\n",
    "for i,img in enumerate(imgs):\n",
    "    imgid = img['id']\n",
    "    \n",
    "    # coco specific here, they store train/val images separately\n",
    "    loc = 'train2014' if 'train' in img['file_name'] else 'val2014'\n",
    "    \n",
    "    jimg = {}\n",
    "    jimg['file_path'] = os.path.join(loc, img['file_name'])\n",
    "    jimg['id'] = imgid\n",
    "    \n",
    "    sents = []\n",
    "    annotsi = itoa[imgid]\n",
    "    for a in annotsi:\n",
    "        sents.append(a['caption'])\n",
    "    jimg['captions'] = sents[0]\n",
    "    out.append(jimg)\n",
    "    \n",
    "json.dump(out, open('coco_raw_vqa.json', 'w'))\n",
    "\n",
    "# lets see what they look like\n",
    "print out[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
